{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac49e53c",
   "metadata": {},
   "source": [
    "**Flight Delay Prediction** by *FlightLogix*\n",
    "@author - ibanerjee32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfca18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp, round, coalesce\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b31c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"FlightDelayPrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86982aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is used for loading CSV files from input path\n",
    "def load_data(data_path, qtr):\n",
    "    \n",
    "    #data_path = \"../Data/data_by_qtr/Q4.csv\"\n",
    "    #df = sqlContext.read.option(\"header\",True) \\\n",
    "    # .csv(data_file_path)\n",
    "    \n",
    "    df = spark.read.option(\"header\", True).csv(data_path)\n",
    "    #df = df.filter(col(\"Quarter\") == 4)#should we change it to month later\n",
    "    dp_cnt = df.count()\n",
    "    attr_cnt = len(df.columns)\n",
    "    print(\"Total Number of datapoints :{} and the total number of attributes :{}\".format(dp_cnt, attr_cnt))\n",
    "    \n",
    "    \n",
    "    attributes = df.columns\n",
    "    # Convert the column names to a list \n",
    "    attributes_list = list(attributes)\n",
    "\n",
    "    # Print the column names\n",
    "    print(\"attributes are\", attributes_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037229a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, isPredict):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with a filtered and cleaned subset of attributes\n",
    "    '''\n",
    "   \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.types import IntegerType, FloatType, TimestampType\n",
    "    \n",
    "    filtered_df=df.select(['Year','Month', 'Quarter', 'DayofMonth', 'DayOfWeek', 'Origin', 'Dest',  'ScheduledDepartureTime', \\\n",
    "                           'ScheduledArrivalTime',  'ArrDelayMinutes', 'Cancelled',  \\\n",
    "                           'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay',\\\n",
    "                           'LateAircraftDelay', 'AirlineCommonName']) \n",
    "    \n",
    "    # Create a list of column names in the DataFrame\n",
    "    columns = filtered_df.columns\n",
    "\n",
    "    # Use a list comprehension to count null values in each column\n",
    "    null_counts = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n",
    "\n",
    "    # Select the null counts for each column\n",
    "    null_counts_df = filtered_df.select(null_counts)\n",
    "\n",
    "    # Show the result\n",
    "    null_counts_df.show()\n",
    "    \n",
    "    nan_pattern = '\"\"\"NaN\"\"\"'\n",
    "    # Check for missing (NaN) and \"NaN\" values\n",
    "    missing_values = filtered_df.select([count(when(isnan(col) | (trim(col) == lit(nan_pattern)), col)).alias(col)\\\n",
    "                                         for col in filtered_df.columns])\n",
    "\n",
    "    # Create a list of column names with missing values\n",
    "    columns_with_missing_values = [col_name for col_name in missing_values.columns \\\n",
    "                                   if missing_values.select(col_name).first()[0] > 0]\n",
    "\n",
    "    # Print the column names with missing values\n",
    "    if columns_with_missing_values:\n",
    "        print(\"Columns with missing values (NaN or 'NaN'):\")\n",
    "        for col_name in columns_with_missing_values:\n",
    "            print(col_name)\n",
    "            # Iterate through the columns and replace missing values\n",
    "            filtered_df = filtered_df.withColumn(col_name, when((trim(col(col_name)) == lit(nan_pattern)), 0)\\\n",
    "                                              .otherwise(col(col_name)))\n",
    "            \n",
    "    else:\n",
    "        print(\"No missing values (NaN or 'NaN') found in any column.\")\n",
    "    \n",
    "    # Drop rows with any null values\n",
    "    if not isPredict:\n",
    "        filtered_df = filtered_df.na.drop()\n",
    "    \n",
    "    \n",
    "    # Convert \"ScheduledArrivalTime\" to a string column and add a leading zero if necessary\n",
    "    filtered_df = filtered_df.withColumn(\"ScheduledArrivalTime\", expr(\"LPAD(ScheduledArrivalTime, 4, '0')\"))\\\n",
    "                             .withColumn(\"ScheduledDepartureTime\", expr(\"LPAD(ScheduledDepartureTime, 4, '0')\"))\n",
    "\n",
    "    # Extract the hour component from \"ScheduledArrivalTime\"\n",
    "    filtered_df = filtered_df.withColumn(\"ArrivalHour\", substring(\"ScheduledArrivalTime\", 1, 2))\\\n",
    "                             .withColumn(\"DepartureHour\", substring(\"ScheduledDepartureTime\", 1, 2))\n",
    "\n",
    "    \n",
    "    # Define time ranges and labels for the categorical feature\n",
    "    time_ranges = [(0, 11, \"morning\"), (12, 17, \"afternoon\"), (18, 23, \"evening\")]\n",
    "\n",
    "    # Create binary columns for each time of day category\n",
    "    for start, end, label in time_ranges:\n",
    "        filtered_df = filtered_df.withColumn(\"Arrtimeofday_\" + label, when(\n",
    "            (col(\"ArrivalHour\").cast(\"int\") >= start) & (col(\"ArrivalHour\").cast(\"int\") <= end),\n",
    "            1\n",
    "        ).otherwise(0))\n",
    "\n",
    "        filtered_df = filtered_df.withColumn(\"Deptimeofday_\" + label, when(\n",
    "            (col(\"DepartureHour\").cast(\"int\") >= start) & (col(\"DepartureHour\").cast(\"int\") <= end),\n",
    "            1\n",
    "        ).otherwise(0))\n",
    "    \n",
    "    \n",
    "    # Add a new column \"WeekOfMonth\" based on \"DayofMonth\"\n",
    "    # The number of weeks is calculated based on the integer division by 7.\n",
    "    filtered_df = filtered_df.withColumn(\"WeekOfMonth\", (filtered_df[\"DayofMonth\"] / 7).cast(\"int\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define a dictionary of column names and their corresponding data types\n",
    "    column_types = {\n",
    "        \"Year\": IntegerType(),\n",
    "        \"Month\": IntegerType(),\n",
    "        \"Quarter\":IntegerType(),\n",
    "        \"DayofMonth\": IntegerType(),\n",
    "        \"WeekOfMonth\": IntegerType(),\n",
    "        \"DayOfWeek\": IntegerType(),\n",
    "        #\"Origin\": StringType(),\n",
    "        #\"Dest\": StringType(),\n",
    "        \"ScheduledDepartureTime\": IntegerType(),\n",
    "        \"ScheduledArrivalTime\": IntegerType(),\n",
    "        \"ArrDelayMinutes\": IntegerType(),\n",
    "        \"Cancelled\": IntegerType(),\n",
    "        \"CarrierDelay\": IntegerType(),\n",
    "        \"WeatherDelay\":IntegerType(),\n",
    "        \"NASDelay\": IntegerType(),\n",
    "        \"SecurityDelay\":IntegerType(),\n",
    "        \"LateAircraftDelay\":IntegerType()\n",
    "        #\"AirlineCommonName\": StringType()\n",
    "    }\n",
    "\n",
    "    # Apply the conversions using a loop\n",
    "    for column_name, data_type in column_types.items():\n",
    "        filtered_df = filtered_df.withColumn(column_name, col(column_name).cast(data_type))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0a55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_encoding(df):\n",
    "    \n",
    "   \n",
    "    # Define a list of categorical columns to be encoded, add quarter if needed\n",
    "    categorical_cols = [\"Month\"] + \\\n",
    "        [\"DayOfWeek\", \"WeekOfMonth\", \"Origin\", \"Dest\", \"AirlineCommonName\"] + \\\n",
    "        [\"Deptimeofday_morning\", \"Deptimeofday_afternoon\", \"Deptimeofday_evening\"] + \\\n",
    "        [\"Arrtimeofday_morning\", \"Arrtimeofday_afternoon\", \"Arrtimeofday_evening\"]\n",
    "\n",
    "\n",
    "\n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "    encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\") for indexer in indexers]\n",
    "\n",
    "    indexed_df = df\n",
    "    for indexer in indexers:\n",
    "        indexed_df = indexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    encoded_df = indexed_df\n",
    "    for encoder in encoders:\n",
    "        encoded_df = encoder.fit(encoded_df).transform(encoded_df)\n",
    "\n",
    "    # Define feature columns and create a feature vector, #\"Quarter_index_encoded\",\n",
    "    feature_columns = [\"Month_index_encoded\", \\\n",
    "                       \"DayOfWeek_index_encoded\", \"WeekOfMonth_index_encoded\",\\\n",
    "                   \"Deptimeofday_morning_index_encoded\", \"Deptimeofday_afternoon_index_encoded\", \"Deptimeofday_evening_index_encoded\",\\\n",
    "                   \"Arrtimeofday_morning_index_encoded\", \"Arrtimeofday_afternoon_index_encoded\", \"Arrtimeofday_evening_index_encoded\",\\\n",
    "                   \"Origin_index_encoded\", \"Dest_index_encoded\", \"AirlineCommonName_index_encoded\"]\n",
    "    vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    \n",
    "    df = vector_assembler.transform(encoded_df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9f0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeData(df):\n",
    "    result_df = df.groupBy('Month', 'Year')\\\n",
    "    .agg(\n",
    "        sum(col('ArrDelayMinutes')).alias('Total_Delay_In_Mins'),\n",
    "        sum(col('ArrivalDelayed')).alias('Total_Delay'),\n",
    "        sum((col('Cancelled') == 1).cast('int')).alias('Total_Cancelled'),\n",
    "        sum((col('ArrivalDelayed') == 1).cast('int')).alias('Sum_ArrivalDelayed_1s'),\n",
    "        sum((col('ArrivalDelayed') == 0).cast('int')).alias('Count_ArrivalDelayed_0s'),\n",
    "        sum((col('ArrivalDelayed') == 1).cast('int') + (col('ArrivalDelayed') == 0).cast('int')).alias('Total_Arrivals')\n",
    "    )\\\n",
    "    .orderBy(col('Month').desc(), col('Year').desc(), col('Total_Delay').desc())\n",
    "\n",
    "    # Calculate delay and cancelled percentage\n",
    "    result_df = result_df.withColumn('Delay_Percentage_NC', (col('Total_Delay') / (col('Total_Arrivals') - col('Total_Cancelled'))) * 100)\n",
    "    result_df = result_df.withColumn('Cancelled_Percentage', (col('Total_Cancelled') / col('Total_Arrivals')) * 100)\n",
    "\n",
    "    # Select the desired columns\n",
    "    result_df = result_df.select('Month', 'Year', 'Total_Delay_In_Mins', 'Total_Delay', 'Total_Cancelled', 'Total_Arrivals', 'Delay_Percentage_NC', 'Cancelled_Percentage')\n",
    "\n",
    "    result_df.show(25)\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate the percentage of 'CarrierDelayed' instances\n",
    "    total_count = df.count()\n",
    "    \n",
    "    arrival_delayed_count = df.filter(col(\"ArrivalDelayed\") == 1).count()\n",
    "\n",
    "    # Calculate the percentage\n",
    "    arrival_delayed_percentage = (arrival_delayed_count / total_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'ArrivalDelayed' instances: {arrival_delayed_percentage:.2f}%\")\n",
    "    \n",
    "    carrier_delayed_count = df.filter(col(\"CarrierDelayed\") == 1).count()\n",
    "\n",
    "    # Calculate the percentage\n",
    "    carrier_delayed_percentage = (carrier_delayed_count / arrival_delayed_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'CarrierDelayed' instances: {carrier_delayed_percentage:.2f}%\")\n",
    "    \n",
    "    weather_delayed_count = df.filter(col(\"WeatherDelayed\") == 1).count()\n",
    "\n",
    "    # Calculate the percentage\n",
    "    weather_delayed_percentage = (weather_delayed_count / arrival_delayed_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'WeatherDelayed' instances: {weather_delayed_percentage:.2f}%\")\n",
    "    \n",
    "    nas_delayed_count = df.filter(col(\"NASDelayed\") == 1).count()\n",
    "    # Calculate the percentage\n",
    "    nas_delayed_percentage = (nas_delayed_count / arrival_delayed_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'NASDelayed' instances: {nas_delayed_percentage:.2f}%\")\n",
    "    \n",
    "    security_delayed_count = df.filter(col(\"SecurityDelayed\") == 1).count()\n",
    "\n",
    "    # Calculate the percentage\n",
    "    security_delayed_percentage = (security_delayed_count / arrival_delayed_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'SecurityDelayed' instances: {security_delayed_percentage:.2f}%\")\n",
    "    \n",
    "    lateAircraft_delayed_count = df.filter(col(\"LateAircraftDelay\") == 1).count()\n",
    "    # Calculate the percentage\n",
    "    lateAircraft_delayed_percentage = (lateAircraft_delayed_count / arrival_delayed_count) * 100\n",
    "\n",
    "    print(f\"Percentage of 'LateAircraftDelay' instances: {lateAircraft_delayed_percentage:.2f}%\")\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56186f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRegression(train_data, test_data, regressor, isPredict):    \n",
    "\n",
    "    # Fit the model to your training data\n",
    "    reg_model = regressor.fit(train_data)\n",
    "    \n",
    "    if not isPredict:\n",
    "        # Make predictions\n",
    "        reg_predictions = reg_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model using Mean Absolute Error (MAE)\n",
    "        evaluator = RegressionEvaluator(labelCol=\"ArrDelayMinutes\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        mae = evaluator.evaluate(reg_predictions)\n",
    "        print(\"Regressor - Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "        # Evaluate the model using R-squared\n",
    "        evaluator = RegressionEvaluator(labelCol=\"ArrDelayMinutes\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        r_squared = evaluator.evaluate(reg_predictions)\n",
    "        print(' R-squared Score:', r_squared)\n",
    "    \n",
    "    return reg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b323c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performClassification(train_data, test_data, classifier, resp):    \n",
    "\n",
    "\n",
    "\n",
    "    # Fit the model to  data\n",
    "    model = classifier.fit(train_data)\n",
    "\n",
    "    # use the model to make predictions on new data\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Create a BinaryClassificationEvaluator\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=resp)\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    area_under_roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    area_under_pr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(\"Area Under ROC: {:.2f}\".format(area_under_roc))\n",
    "    print(\"Area Under PR: {:.2f}\".format(area_under_pr))\n",
    "    \n",
    "       \n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = predictions.filter(expr('prediction = ' +resp))\n",
    "    accuracy = correct_predictions.count() / predictions.count()\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "    # Calculate precision\n",
    "    true_positives = correct_predictions.filter(expr(resp+' = 1'))\n",
    "    false_positives = predictions.filter(expr('(prediction = 1) AND (' +resp+' = 0)'))\n",
    "    precision = true_positives.count() / (true_positives.count() + false_positives.count())\n",
    "    print(\"Precision: {:.2f}\".format(precision))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854ec679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRandomForestClassification(train_data, test_data):\n",
    "    # Define the Random Forest model\n",
    "    rf = RandomForestClassifier(labelCol=\"ArrivalDelayed\", featuresCol=\"features\", numTrees=100, maxDepth=6)\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"ArrivalDelayed\")\n",
    "    area_under_roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    area_under_pr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    print(\"Random Forest - Area Under ROC: {:.2f}\".format(area_under_roc))\n",
    "    print(\"Random Forest - Area Under PR: {:.2f}\".format(area_under_pr))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct_predictions = predictions.filter(expr('prediction = ArrivalDelayed'))\n",
    "    accuracy = correct_predictions.count() / predictions.count()\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "    # Calculate precision\n",
    "    true_positives = correct_predictions.filter(expr('ArrivalDelayed = 1'))\n",
    "    false_positives = predictions.filter(expr('(prediction = 1) AND (ArrivalDelayed = 0)'))\n",
    "    precision = true_positives.count() / (true_positives.count() + false_positives.count())\n",
    "    print(\"Precision: {:.2f}\".format(precision))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b612bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performGBClassification(train_data, test_data, resp, isPredict):\n",
    "    from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "    # Define the Gradient Boosting model\n",
    "    gbt = GBTClassifier(labelCol=resp, featuresCol=\"features\", maxIter=10, maxDepth=5, stepSize=0.1)\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "    # Fit the model to training data\n",
    "    model = pipeline.fit(train_data)\n",
    "    \n",
    "    if not isPredict:\n",
    "        # Make predictions on the test data\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=resp)\n",
    "        area_under_roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "        area_under_pr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "        print(\"Gradient Boosting - Area Under ROC: {:.2f}\".format(area_under_roc))\n",
    "        print(\"Gradient Boosting - Area Under PR: {:.2f}\".format(area_under_pr))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_predictions = predictions.filter(expr('prediction = ' +resp))\n",
    "        accuracy = correct_predictions.count() / predictions.count()\n",
    "        print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "        # Calculate precision\n",
    "        true_positives = correct_predictions.filter(expr(resp+' = 1'))\n",
    "        false_positives = predictions.filter(expr('(prediction = 1) AND (' +resp+' = 0)'))\n",
    "        precision = true_positives.count() / (true_positives.count() + false_positives.count())\n",
    "        print(\"Precision: {:.2f}\".format(precision))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3242ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of datapoints :7838605 and the total number of attributes :53\n",
      "attributes are ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Marketing_Airline_Network', 'DOT_ID_Marketing_Airline', 'Flight_Number_Marketing_Airline', 'Operating_Airline', 'Tail_Number', 'OriginAirportID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'DestAirportID', 'DestCityMarketID', 'Dest', 'DestCityName', 'ScheduledDepartureTime', 'ActualDepartureTime', 'DepDelay', 'DepartureDelayGroups', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'ScheduledArrivalTime', 'ActualArrivalTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrivalDelayGroups', 'Cancelled', 'CancellationCode', 'Diverted', 'ScheduledElapseTime', 'ActualElapsedTime', 'AirTime', 'Distance', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime', 'DivArrDelay', 'DivDistance', 'Duplicate', 'AirlineCommonName']\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|Year|Month|Quarter|DayofMonth|DayOfWeek|Origin|Dest|ScheduledDepartureTime|ScheduledArrivalTime|ArrDelayMinutes|Cancelled|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|AirlineCommonName|\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|   0|    0|      0|         0|        0|     0|   0|                     0|                   0|              0|        0|           0|           0|       0|            0|                0|                0|\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "\n",
      "No missing values (NaN or 'NaN') found in any column.\n",
      "Total Number of datapoints :1425161 and the total number of attributes :54\n",
      "attributes are ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Marketing_Airline_Network', 'DOT_ID_Marketing_Airline', 'Flight_Number_Marketing_Airline', 'Operating_Airline', 'Tail_Number', 'OriginAirportID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'DestAirportID', 'DestCityMarketID', 'Dest', 'DestCityName', 'ScheduledDepartureTime', 'ActualDepartureTime', 'DepDelay', 'DepartureDelayGroups', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'ScheduledArrivalTime', 'ActualArrivalTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrivalDelayGroups', 'Cancelled', 'CancellationCode', 'Diverted', 'ScheduledElapseTime', 'ActualElapsedTime', 'AirTime', 'Distance', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime', 'DivArrDelay', 'DivDistance', 'Duplicate', 'AirlineCommonName', 'Date']\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|Year|Month|Quarter|DayofMonth|DayOfWeek|Origin|Dest|ScheduledDepartureTime|ScheduledArrivalTime|ArrDelayMinutes|Cancelled|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|AirlineCommonName|\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|   0|    0|      0|         0|        0|     0|   0|                     0|                   0|        1425161|  1425161|     1425161|     1425161| 1425161|      1425161|          1425161|                0|\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "\n",
      "No missing values (NaN or 'NaN') found in any column.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../Data/data_by_qtr/Q1.csv\"\n",
    "Qtr=1\n",
    "source_df=load_data(data_path, Qtr)\n",
    "isPredict=False\n",
    "source_df=clean_data(source_df, isPredict)\n",
    "\n",
    "\n",
    "test_data_path = \"../Data/data_by_qtr/template-2024_Q1.csv\"\n",
    "Qtr=1\n",
    "unseen_df=load_data(test_data_path, Qtr)\n",
    "isPredict=True\n",
    "unseen_df=clean_data(unseen_df,isPredict )\n",
    "\n",
    "# Adding a 'Dataset' column to distinguish between source and unseen datasets\n",
    "source_df = source_df.withColumn(\"Dataset\", lit(\"source\"))\n",
    "unseen_df = unseen_df.withColumn(\"Dataset\", lit(\"unseen\"))\n",
    "\n",
    "# Combine the source and unseen data\n",
    "combined_df = source_df.union(unseen_df)\n",
    "combined_df=apply_encoding(combined_df)\n",
    "\n",
    "# Separate back into source and unseen data\n",
    "source_df = combined_df.filter(col(\"Dataset\") == \"source\").drop(\"Dataset\")\n",
    "unseen_df = combined_df.filter(col(\"Dataset\") == \"unseen\").drop(\"Dataset\")\n",
    "\n",
    "# Create  new binary columns for predictions\n",
    "source_df = source_df.withColumn(\"ArrivalDelayed\", when(col(\"ArrDelayMinutes\") > 0, 1).otherwise(0)) \n",
    "source_df = source_df.withColumn(\"CarrierDelayed\", when(col(\"CarrierDelay\") > 0, 1).otherwise(0))\n",
    "source_df = source_df.withColumn(\"WeatherDelayed\", when(col(\"WeatherDelay\") > 0, 1).otherwise(0))\n",
    "source_df = source_df.withColumn(\"NASDelayed\", when(col(\"NASDelay\") > 0, 1).otherwise(0))\n",
    "source_df = source_df.withColumn(\"SecurityDelayed\", when(col(\"SecurityDelay\") > 0, 1).otherwise(0))\n",
    "source_df = source_df.withColumn(\"LateAircraftDelayed\", when(col(\"LateAircraftDelay\") > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22159f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performTheilUTest(df):\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "    # \"DayOfWeek_index_encoded\", \"WeekOfMonth_index_encoded\",\\\n",
    "    categorical_features = [\"Month_index_encoded\", \\\n",
    "                      \n",
    "                   \"Deptimeofday_morning_index_encoded\", \"Deptimeofday_afternoon_index_encoded\", \"Deptimeofday_evening_index_encoded\",\\\n",
    "                   \"Arrtimeofday_morning_index_encoded\", \"Arrtimeofday_afternoon_index_encoded\", \"Arrtimeofday_evening_index_encoded\",\\\n",
    "                   \"Origin_index_encoded\", \"Dest_index_encoded\", \"AirlineCommonName_index_encoded\"]\n",
    "    # Create an empty DataFrame to store the results\n",
    "    # Create an empty DataFrame to store the results with schema\n",
    "    schema = StructType([\n",
    "        StructField(\"Feature1\", StringType(), False),\n",
    "        StructField(\"Feature2\", StringType(), False),\n",
    "        StructField(\"Theil's U\", DoubleType(), False)\n",
    "    ])\n",
    "    results_df = spark.createDataFrame([], schema)\n",
    "\n",
    "    for feature1 in categorical_features:\n",
    "        for feature2 in categorical_features:\n",
    "            if feature1 != feature2:\n",
    "                # Calculate the Theil's U value\n",
    "                u = df.groupBy(feature1, feature2).count()\n",
    "                total_counts = u.groupBy(feature1).agg(F.sum(\"count\").alias(\"total\"))\n",
    "                u = u.join(total_counts, feature1, \"left\")\n",
    "                u = u.withColumn(\"p_x\", F.col(\"count\") / F.col(\"total\"))\n",
    "                p_y_given_x = Window.partitionBy(feature1)\n",
    "                u = u.withColumn(\"p_y_given_x\", F.sum(\"count\").over(p_y_given_x) / F.col(\"total\"))\n",
    "                u = u.withColumn(\"u\", F.col(\"p_x\") * F.log(F.col(\"p_x\") / F.col(\"p_y_given_x\")))\n",
    "                u = u.groupBy(feature1, feature2).agg(F.sum(\"u\").alias(\"u\"))\n",
    "                u.show(1)\n",
    "                u = u.withColumn(\"u_max\", F.log(F.countDistinct(F.col(feature2)).cast(\"double\")))\n",
    "                u = u.withColumn(\"theil_u\", (u[\"u_max\"] - u[\"u\"]) / u[\"u_max\"])\n",
    "\n",
    "                # Append the results to the results DataFrame\n",
    "                results_df = results_df.unionAll(spark.createDataFrame([(feature1, feature2, u.select(\"theil_u\").first()[\"theil_u\"])], results_df.schema))\n",
    "\n",
    "    # Show the results\n",
    "    results_df.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc49b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performChiSquaredTest(df):\n",
    "    # Define feature columns and create a feature vector, #\"Quarter_index_encoded\",\n",
    "    feature_columns = [\"Month_index_encoded\", \\\n",
    "                       \"DayOfWeek_index_encoded\", \"WeekOfMonth_index_encoded\",\\\n",
    "                   \"Deptimeofday_morning_index_encoded\", \"Deptimeofday_afternoon_index_encoded\", \"Deptimeofday_evening_index_encoded\",\\\n",
    "                   \"Arrtimeofday_morning_index_encoded\", \"Arrtimeofday_afternoon_index_encoded\", \"Arrtimeofday_evening_index_encoded\",\\\n",
    "                   \"Origin_index_encoded\", \"Dest_index_encoded\", \"AirlineCommonName_index_encoded\"]\n",
    "    # Perform the Chi-Square test for each categorical feature\n",
    "    chi_square_results = []\n",
    "\n",
    "    for feature in feature_columns:\n",
    "        chiSqResult = ChiSquareTest.test(df1, feature, \"ArrivalDelayed\")\n",
    "        chi_square_results.append((feature, chiSqResult))\n",
    "\n",
    "    # Print the results\n",
    "    for feature, result in chi_square_results:\n",
    "        print(f\"Chi-Square Test for Feature: {feature}\")\n",
    "        result.show(truncate=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "077fd927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzeData(source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77f1a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Qtr 1 and Step 1, uncomment the following\n",
    "#source_df = source_df.filter(~((col(\"Year\") == 2020) & (col(\"Month\") == 3)))\n",
    "#for Qtr 2 and Step 1, uncomment the following\n",
    "#source_df = source_df.filter(~((col(\"Year\") == 2020) & (col(\"Month\") == 4)))\n",
    "#analyzeData(source_df)\n",
    "#comment the following for Step 1\n",
    "source_df = source_df.filter(col(\"Cancelled\") == 0)\n",
    "\n",
    "#following needed for Step 3 onwards\n",
    "#if interested in prediction of delay factors, otherwise comment this line\n",
    "source_df = source_df.filter(col(\"ArrivalDelayed\") == 1)\n",
    "\n",
    "#source_df.show(1)\n",
    "\n",
    "#uncomment to check correlation\n",
    "#performChiSquaredTest(source_df)\n",
    "\n",
    "# Split the data into training and test sets (70% training, 30% test)\n",
    "train_data, test_data = source_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff3450d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to run any specific model other than GBBoosting, GBBoosting performed slightly better\n",
    "'''\n",
    "isPredict=False\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create and train a model (e.g., Linear Regression)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"ArrDelayMinutes\")\n",
    "performRegression(train_data, test_data,lr, isPredict)\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "# Create a Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"ArrDelayMinutes\")\n",
    "performRegression(train_data, test_data,dt, isPredict)\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "# Create a Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"ArrDelayMinutes\")\n",
    "performRegression(train_data, test_data,rf, isPredict)\n",
    "\n",
    "'''\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "# Create a Gradient Boosting Regressor # consider LightGBM\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"ArrDelayMinutes\", maxIter=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b250a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to run any specific model other than GBBoosting, GBBoosting performed slightly better\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Define the logistic regression model\n",
    "#logistic_regression = LogisticRegression(featuresCol=\"features\", labelCol=\"ArrivalDelayed\")\n",
    "#performClassification(train_data, test_data,logistic_regression, 'ArrivalDelayed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b5fe955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Define the Define the DT model\n",
    "#dtClassifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"ArrivalDelayed\")\n",
    "#performClassification(train_data, test_data,dtClassifier, 'ArrivalDelayed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "291cb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performRandomForestClassification(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0d5a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def extract_prob(v):\n",
    "    try:\n",
    "        return float(v[1])  # VectorUDT is of length 2\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "#get_cancelled_prob = udf(extract_prob, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42ae678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+-----------+-------------+--------------------+--------------------+----------------------+----------------------+--------------------+--------------------+-----------+-----------+---------------+-----------------+------------+----------+-----------------------+--------------------------+----------------------------+--------------------------+--------------------------+----------------------------+--------------------------+-------------------+-----------------------+-------------------------+--------------------+------------------+-------------------------------+----------------------------------+------------------------------------+----------------------------------+----------------------------------+------------------------------------+----------------------------------+-------------------------------------------------------------------------------+------------------+\n",
      "|Year|Month|Quarter|DayofMonth|DayOfWeek|Origin|Dest|ScheduledDepartureTime|ScheduledArrivalTime|ArrDelayMinutes|Cancelled|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|AirlineCommonName|ArrivalHour|DepartureHour|Arrtimeofday_morning|Deptimeofday_morning|Arrtimeofday_afternoon|Deptimeofday_afternoon|Arrtimeofday_evening|Deptimeofday_evening|WeekOfMonth|Month_index|DayOfWeek_index|WeekOfMonth_index|Origin_index|Dest_index|AirlineCommonName_index|Deptimeofday_morning_index|Deptimeofday_afternoon_index|Deptimeofday_evening_index|Arrtimeofday_morning_index|Arrtimeofday_afternoon_index|Arrtimeofday_evening_index|Month_index_encoded|DayOfWeek_index_encoded|WeekOfMonth_index_encoded|Origin_index_encoded|Dest_index_encoded|AirlineCommonName_index_encoded|Deptimeofday_morning_index_encoded|Deptimeofday_afternoon_index_encoded|Deptimeofday_evening_index_encoded|Arrtimeofday_morning_index_encoded|Arrtimeofday_afternoon_index_encoded|Arrtimeofday_evening_index_encoded|features                                                                       |prediction        |\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+-----------+-------------+--------------------+--------------------+----------------------+----------------------+--------------------+--------------------+-----------+-----------+---------------+-----------------+------------+----------+-----------------------+--------------------------+----------------------------+--------------------------+--------------------------+----------------------------+--------------------------+-------------------+-----------------------+-------------------------+--------------------+------------------+-------------------------------+----------------------------------+------------------------------------+----------------------------------+----------------------------------+------------------------------------+----------------------------------+-------------------------------------------------------------------------------+------------------+\n",
      "|2024|1    |1      |23        |2        |ABY   |ATL |1202                  |1304                |NULL           |NULL     |NULL        |NULL        |NULL    |NULL         |NULL             |Endeavor Air Inc.|13         |12           |0                   |0                   |1                     |1                     |0                   |0                   |3          |1.0        |5.0            |0.0              |266.0       |0.0       |7.0                    |0.0                       |1.0                         |0.0                       |0.0                       |1.0                         |0.0                       |(2,[1],[1.0])      |(6,[5],[1.0])          |(4,[0],[1.0])            |(382,[266],[1.0])   |(382,[0],[1.0])   |(27,[7],[1.0])                 |(1,[0],[1.0])                     |(1,[],[])                           |(1,[0],[1.0])                     |(1,[0],[1.0])                     |(1,[],[])                           |(1,[0],[1.0])                     |(809,[1,7,8,12,14,15,17,284,400,789],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|45.016087029899886|\n",
      "|2024|1    |1      |24        |3        |ABY   |ATL |1202                  |1304                |NULL           |NULL     |NULL        |NULL        |NULL    |NULL         |NULL             |Endeavor Air Inc.|13         |12           |0                   |0                   |1                     |1                     |0                   |0                   |3          |1.0        |4.0            |0.0              |266.0       |0.0       |7.0                    |0.0                       |1.0                         |0.0                       |0.0                       |1.0                         |0.0                       |(2,[1],[1.0])      |(6,[4],[1.0])          |(4,[0],[1.0])            |(382,[266],[1.0])   |(382,[0],[1.0])   |(27,[7],[1.0])                 |(1,[0],[1.0])                     |(1,[],[])                           |(1,[0],[1.0])                     |(1,[0],[1.0])                     |(1,[],[])                           |(1,[0],[1.0])                     |(809,[1,6,8,12,14,15,17,284,400,789],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|45.016087029899886|\n",
      "+----+-----+-------+----------+---------+------+----+----------------------+--------------------+---------------+---------+------------+------------+--------+-------------+-----------------+-----------------+-----------+-------------+--------------------+--------------------+----------------------+----------------------+--------------------+--------------------+-----------+-----------+---------------+-----------------+------------+----------+-----------------------+--------------------------+----------------------------+--------------------------+--------------------------+----------------------------+--------------------------+-------------------+-----------------------+-------------------------+--------------------+------------------+-------------------------------+----------------------------------+------------------------------------+----------------------------------+----------------------------------+------------------------------------+----------------------------------+-------------------------------------------------------------------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "isPredict=True\n",
    "#Step1 - Predict Cancellation\n",
    "#model=performGBClassification(train_data, test_data,'Cancelled',isPredict )\n",
    "#Step2 - Predict ArrivalDelayed\n",
    "#model=performGBClassification(train_data, test_data,'ArrivalDelayed',isPredict )\n",
    "#Step3 - Predict CarrierDelayed\n",
    "#model=performGBClassification(train_data, test_data,'CarrierDelayed',isPredict )\n",
    "#Step4 - Predict WeatherDelayed\n",
    "#model=performGBClassification(train_data, test_data,'WeatherDelayed',isPredict )\n",
    "#Step5 - Predict NASDelayed\n",
    "#model=performGBClassification(train_data, test_data,'NASDelayed',isPredict )\n",
    "#Step6 - Predict ArrDelayMinutes\n",
    "model=performRegression(train_data, test_data,gbt, isPredict)\n",
    "\n",
    "source_df=None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract the probability of class 1\n",
    "#get_cancelled_prob = udf(lambda probability: str(probability), StringType())\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(unseen_df)\n",
    "\n",
    "predictions.show(2, truncate=False)\n",
    "\n",
    "unseen_df=None\n",
    "# Extract the probability of class 1 and assign it to 'Cancelled' column\n",
    "#predictions = predictions.withColumn('Cancelled_Prob', get_cancelled_prob(col('probability')))\n",
    "\n",
    "#uncomment below 2 lines for Step 1 - 5\n",
    "'''\n",
    "predictions=predictions.select(['Year', 'Month', 'Quarter', 'DayofMonth', 'DayOfWeek', 'Origin', 'Dest', \\\n",
    "                                'ScheduledDepartureTime', 'ScheduledArrivalTime', 'ArrDelayMinutes',  \\\n",
    "                                'probability','prediction'])\n",
    "predictions = predictions.withColumn('probability', col('probability').cast(StringType()))\n",
    "'''\n",
    "\n",
    "#uncomment for Step 1\n",
    "#predictions.coalesce(1).write.csv('Q1_cancel_pred_output', header=True)\n",
    "\n",
    "#uncomment for Step 2\n",
    "#predictions.coalesce(1).write.csv('Q1_arrival_delay_pred_output', header=True)\n",
    "\n",
    "#uncomment for Step 3\n",
    "#predictions.coalesce(1).write.csv('Q1_carrier_delay_pred_output', header=True)\n",
    "\n",
    "#uncomment for Step 4\n",
    "#predictions.coalesce(1).write.csv('Q1_weather_delay_pred_output', header=True)\n",
    "\n",
    "#uncomment for Step 5\n",
    "#predictions.coalesce(1).write.csv('Q1_nas_delay_pred_output', header=True)\n",
    "\n",
    "\n",
    "#uncomment for Step 6\n",
    "\n",
    "predictions=predictions.select(['Year', 'Month', 'Quarter', 'DayofMonth', 'DayOfWeek', 'Origin', 'Dest', \\\n",
    "                                'ScheduledDepartureTime', 'ScheduledArrivalTime', 'ArrDelayMinutes',  \\\n",
    "                                'prediction'])\n",
    "predictions.coalesce(1).write.csv('Q1_arr_delay_in_mins_pred_output', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a42ca096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
